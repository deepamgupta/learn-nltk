{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Module\n",
    "\n",
    "in this video we're going\n",
    "to be talking about importing a new\n",
    "dataset so why are we going to import a\n",
    "new dataset one you should get\n",
    "comfortable with how to do it to the\n",
    "other dataset wasn't working with where\n",
    "we want to move in the future\n",
    "\n",
    "> This section also contains **saving this file** in python format with the name `sentiment_mod.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random # this we will use to shuffle our dataset, coz for now it is higlhy ordered, first all negative, than positive than neutaral.\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "# this is basically an nltk module but it is actually a wrapper around the sklearn classifier to add its functionilities in nltk\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every single sklearn learn algorithm comes paired with a bunch of parameters you can use. The default parameters are general parameters that are pretty good but it's good to know what the parameters are and how adjusting them will change your success. \n",
    "\n",
    ">So, keep that in mind that despite the success that we get with these parameter whether it's good or bad you can probably improve that success by at least ten percent by correctly choosing and not using the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining algos with a vote\n",
    "from nltk.classify import ClassifierI # this is so we can inherit from the \"nltk classifier\" class\n",
    "from statistics import mode # this is how we are gonna choose who got the most votes, we're just gonna take the mode\n",
    "# The mode of a set of data values is the value that appears most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for classifier in self._classifiers:\n",
    "            vote = classifier.classify(features)\n",
    "            votes.append(vote)\n",
    "        \n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for classifier in self._classifiers:\n",
    "            vote = classifier.classify(features)\n",
    "            votes.append(vote)\n",
    "        \n",
    "        choice_votes = votes.count(mode(votes)) # how many occurences of most popular votes in that list\n",
    "        conf = choice_votes / len(votes) # this is give us certainity\n",
    "        \n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_pos = open('short_reviews/positive.txt', 'r').read()\n",
    "short_neg = open('short_reviews/negative.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "documents = []\n",
    "\n",
    "# j is adjective, r is adverb, and v is verb\n",
    "# allowed_word_types = ['J', 'R', 'V']\n",
    "\n",
    "allowed_word_types = ['J'] # play with this, you can add 'R' and 'V' too\n",
    "for p in short_pos.split('\\n'):\n",
    "    documents.append((p, 'pos'))\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos: # each w will be a tuple -> (word, pos_tag)\n",
    "        if w[1][0] in allowed_word_types: # w[1] is the pos_tag and w[1][0] is the first letter of that pos_tag\n",
    "            all_words.append(w[0].lower())\n",
    "for p in short_neg.split('\\n'):\n",
    "    documents.append((p, 'neg'))\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos: # each w will be a tuple -> (word, pos_tag)\n",
    "        if w[1][0] in allowed_word_types: # w[1] is the pos_tag and w[1][0] is the first letter of that pos_tag\n",
    "            all_words.append(w[0].lower())\n",
    "          \n",
    "# pickle documents        \n",
    "save_documents = open('pickled_algos/documents.pickle', 'wb')\n",
    "pickle.dump(documents, save_documents)\n",
    "save_documents.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_what we're going to end up having to do is take all of these words though literally take every word in every review and compile them and then basically we do is we'll take that list of words and we'll find the most popular words  used and then we take of those most popular words which one appear in positive text and which ones negative text and then we simply just search for those words and whichever one it has more negative words or more positive words that's how we classify it_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:5000]\n",
    "\n",
    "# pickle word features\n",
    "save_word_features = open('pickled_algos/word_features5k.pickle', 'wb')\n",
    "pickle.dump(word_features, save_word_features)\n",
    "save_word_features.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in words:\n",
    "        features[w] = (w in words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = featuresets[:10000]\n",
    "test_set = featuresets[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Byes Algorithm\n",
    "posterior = prior occurences * occurences / likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes algo accuracy:  75.45180722891565\n",
      "Most Informative Features\n",
      "               wonderful = True              pos : neg    =     21.1 : 1.0\n",
      "              engrossing = True              pos : neg    =     20.5 : 1.0\n",
      "                captures = True              pos : neg    =     17.8 : 1.0\n",
      "                 generic = True              neg : pos    =     16.9 : 1.0\n",
      "                mediocre = True              neg : pos    =     16.2 : 1.0\n",
      "                 routine = True              neg : pos    =     15.6 : 1.0\n",
      "                    dull = True              neg : pos    =     15.3 : 1.0\n",
      "                    flat = True              neg : pos    =     14.5 : 1.0\n",
      "               inventive = True              pos : neg    =     14.4 : 1.0\n",
      "              refreshing = True              pos : neg    =     13.8 : 1.0\n",
      "                      90 = True              neg : pos    =     12.9 : 1.0\n",
      "                  boring = True              neg : pos    =     12.9 : 1.0\n",
      "                    warm = True              pos : neg    =     12.7 : 1.0\n",
      "             mesmerizing = True              pos : neg    =     11.7 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Original Naive Bayes algo accuracy: ', nltk.classify.accuracy(classifier, test_set) * 100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "# pickle classifier\n",
    "save_classifier = open('pickled_algos/originalnaivebayes5k.pickle', 'wb')\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy:  75.30120481927712\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB()) \n",
    "MNB_classifier.train(train_set)\n",
    "print('MNB_classifier accuracy: ', nltk.classify.accuracy(MNB_classifier, test_set) * 100)\n",
    "\n",
    "# pickle MNB_classifier\n",
    "save_classifier = open('pickled_algos/MNB_classifier5k.pickle', 'wb')\n",
    "pickle.dump(MNB_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB_classifier accuracy:  75.75301204819277\n"
     ]
    }
   ],
   "source": [
    "BNB_classifier = SklearnClassifier(BernoulliNB()) \n",
    "BNB_classifier.train(train_set)\n",
    "print('BNB_classifier accuracy: ', nltk.classify.accuracy(BNB_classifier, test_set) * 100)\n",
    "\n",
    "# pickle BNB_classifier\n",
    "save_classifier = open('pickled_algos/BNB_classifier5k.pickle', 'wb')\n",
    "pickle.dump(BNB_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy:  73.3433734939759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression()) \n",
    "LogisticRegression_classifier.train(train_set)\n",
    "print('LogisticRegression_classifier accuracy: ', nltk.classify.accuracy(LogisticRegression_classifier, test_set) * 100)\n",
    "\n",
    "# pickle LogisticRegression_classifier\n",
    "save_classifier = open('pickled_algos/LogisticRegression_classifier5k.pickle', 'wb')\n",
    "pickle.dump(LogisticRegression_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy:  72.7409638554217\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC()) \n",
    "LinearSVC_classifier.train(train_set)\n",
    "print('LinearSVC_classifier accuracy: ', nltk.classify.accuracy(LinearSVC_classifier, test_set) * 100)\n",
    "\n",
    "# pickle LinearSVC_classifier\n",
    "save_classifier = open('pickled_algos/LinearSVC_classifier5k.pickle', 'wb')\n",
    "pickle.dump(LinearSVC_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDC_classifier accuracy:  72.89156626506023\n"
     ]
    }
   ],
   "source": [
    "SGDC_classifier = SklearnClassifier(SGDClassifier()) \n",
    "SGDC_classifier.train(train_set)\n",
    "print('SGDC_classifier accuracy: ', nltk.classify.accuracy(SGDC_classifier, test_set) * 100)\n",
    "\n",
    "# pickle SGDC_classifier\n",
    "save_classifier = open('pickled_algos/SGDC_classifier5k.pickle', 'wb')\n",
    "pickle.dump(SGDC_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC_classifier = SklearnClassifier(SVC()) \n",
    "# SVC_classifier.train(train_set)\n",
    "# print('SVC_classifier accuracy: ', nltk.classify.accuracy(SVC_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC_classifier accuracy:  77.40963855421687\n"
     ]
    }
   ],
   "source": [
    "# NuSVC_classifier = SklearnClassifier(NuSVC()) \n",
    "# NuSVC_classifier.train(train_set)\n",
    "# print('NuSVC_classifier accuracy: ', nltk.classify.accuracy(NuSVC_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNB_classifier = SklearnClassifier(GaussianNB()) \n",
    "# GNB_classifier.train(train_set)\n",
    "# print('GNB_classifier accuracy: ', nltk.classify.accuracy(GNB_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember\n",
    "\n",
    ">These all above algos(classifiers) have got their own parameters, which we can tweak and imrove our accuracy, currently here we have used the default values for each of them.\n",
    ">\n",
    ">If you want to know more about those params, you can check those out on sklearn website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voted_classifier accuracy:  81.17469879518072\n"
     ]
    }
   ],
   "source": [
    "voted_classifier = VoteClassifier(#classifier, \n",
    "                                 MNB_classifier, \n",
    "                                 BNB_classifier,  \n",
    "                                 #SGDClassifier_classifier, \n",
    "                                 LinearSVC_classifier, \n",
    "                                 #NuSVC_classifier,\n",
    "                                 LogisticRegression_classifier)\n",
    "print('voted_classifier accuracy: ', nltk.classify.accuracy(voted_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    feat = find_features(text)\n",
    "    \n",
    "    return voted_classifier.classify(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read these below two lines before moving forward\n",
    "\n",
    "Saved the above whole code as `sentiment_mod.py`.\n",
    "\n",
    "Now using it with custom data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentiment_mod as s # this will take time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pos', 1.0)\n",
      "('neg', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# this will be fast\n",
    "print(s.sentiment(\"This movie was awesome! The acting was great, plot was wonderful and there were pythons... so yea!!!\"))\n",
    "print(s.sentiment(\"This movie was utter junk! There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

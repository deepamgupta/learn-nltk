{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is chunking?\n",
    "\n",
    "So, consider you have a body of text, we know how to \n",
    "1. tokenize and \n",
    "1. how to get pos tags.  \n",
    "\n",
    "#### What should be the next step???\n",
    "\n",
    "Next step is to figure out meaning of a sentence:\n",
    "\n",
    "well first we want to know __who is the sentence talking about__ (i.e. `named entity` or in simple words __noun__)\n",
    "so a person, place or thing is generally going to be your __subject__ (generally)  \n",
    "\n",
    "okay, once you know the named entity what's the next step?\n",
    "\n",
    "the next step is going to be _finding out words that kind of modify or affect that noun_  \n",
    "\n",
    "so you might have many named entities or many nouns in the same sentence\n",
    "e.g.\n",
    "\n",
    "> Apple releases new phone comes with new color case hundred dollars more and Tesla releases home battery\n",
    "\n",
    "okay so this is one sentence but it's about two different things completely and you might even have some opinions in that sentence and you've got to _figure out who or where does that opinion apply_ \n",
    "- is that applying to Apple?\n",
    "- or is that applying the Tesla \n",
    "\n",
    "So most people chunk sentence into what we call as __noun phrases__.\n",
    "It is like a noun surrounded by the words modifying it. \n",
    "\n",
    "So, chunking will output a descriptive group of words surrounding that noun (which we will call a chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union # dataset of presidents state of union from past 70 years\n",
    "from nltk.tokenize import PunktSentenceTokenizer # unsupervised ml sentece_tokenizer, it comes pre trained but we can retrain it for ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sent_tokenize = PunktSentenceTokenizer(train_text) # train the model\n",
    "tokenized = custom_sent_tokenize.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for sent in tokenized:\n",
    "            words = nltk.word_tokenize(sent)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Training Data\n",
    "\n",
    "in this video we're going\n",
    "to be talking about importing a new\n",
    "dataset so why are we going to import a\n",
    "new dataset one you should get\n",
    "comfortable with how to do it to the\n",
    "other dataset wasn't working with where\n",
    "we want to move in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random # this we will use to shuffle our dataset, coz for now it is higlhy ordered, first all negative, than positive than neutaral.\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "# this is basically an nltk module but it is actually a wrapper around the sklearn classifier to add its functionilities in nltk\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every single sklearn learn algorithm comes paired with a bunch of parameters you can use. The default parameters are general parameters that are pretty good but it's good to know what the parameters are and how adjusting them will change your success. \n",
    "\n",
    ">So, keep that in mind that despite the success that we get with these parameter whether it's good or bad you can probably improve that success by at least ten percent by correctly choosing and not using the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining algos with a vote\n",
    "from nltk.classify import ClassifierI # this is so we can inherit from the \"nltk classifier\" class\n",
    "from statistics import mode # this is how we are gonna choose who got the most votes, we're just gonna take the mode\n",
    "# The mode of a set of data values is the value that appears most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for classifier in self._classifiers:\n",
    "            vote = classifier.classify(features)\n",
    "            votes.append(vote)\n",
    "        \n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for classifier in self._classifiers:\n",
    "            vote = classifier.classify(features)\n",
    "            votes.append(vote)\n",
    "        \n",
    "        choice_votes = votes.count(mode(votes)) # how many occurences of most popular votes in that list\n",
    "        conf = choice_votes / len(votes) # this is give us certainity\n",
    "        \n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_pos = open('short_reviews/positive.txt', 'r').read()\n",
    "short_neg = open('short_reviews/negative.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for review in short_pos.split('\\n'):\n",
    "    documents.append((review, 'pos'))\n",
    "for review in short_neg.split('\\n'):\n",
    "    documents.append((review, 'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "short_pos_words = word_tokenize(short_pos)\n",
    "short_neg_words = word_tokenize(short_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in short_pos_words:\n",
    "    all_words.append(word.lower())\n",
    "for word in short_neg_words:\n",
    "    all_words.append(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_what we're going to end up having to do is take all of these words though literally take every word in every review and compile them and then basically we do is we'll take that list of words and we'll find the most popular words  used and then we take of those most popular words which one appear in positive text and which ones negative text and then we simply just search for those words and whichever one it has more negative words or more positive words that's how we classify it_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in words:\n",
    "        features[w] = (w in words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = featuresets[:10000]\n",
    "test_set = featuresets[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Byes Algorithm\n",
    "posterior = prior occurences * occurences / likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes algo accuracy:  81.32530120481928\n",
      "Most Informative Features\n",
      "                    warm = True              pos : neg    =     21.0 : 1.0\n",
      "              engrossing = True              pos : neg    =     18.3 : 1.0\n",
      "                    thin = True              neg : pos    =     17.7 : 1.0\n",
      "                captures = True              pos : neg    =     16.3 : 1.0\n",
      "                    dull = True              neg : pos    =     15.9 : 1.0\n",
      "                 generic = True              neg : pos    =     15.7 : 1.0\n",
      "                 routine = True              neg : pos    =     15.7 : 1.0\n",
      "                    flat = True              neg : pos    =     14.6 : 1.0\n",
      "               inventive = True              pos : neg    =     14.3 : 1.0\n",
      "                mediocre = True              neg : pos    =     14.3 : 1.0\n",
      "              refreshing = True              pos : neg    =     14.3 : 1.0\n",
      "                  boring = True              neg : pos    =     14.1 : 1.0\n",
      "                      90 = True              neg : pos    =     13.7 : 1.0\n",
      "               wonderful = True              pos : neg    =     11.8 : 1.0\n",
      "                mindless = True              neg : pos    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Original Naive Bayes algo accuracy: ', nltk.classify.accuracy(classifier, test_set) * 100)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy:  82.83132530120481\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB()) \n",
    "MNB_classifier.train(train_set)\n",
    "print('MNB_classifier accuracy: ', nltk.classify.accuracy(MNB_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNB_classifier = SklearnClassifier(GaussianNB()) \n",
    "# GNB_classifier.train(train_set)\n",
    "# print('GNB_classifier accuracy: ', nltk.classify.accuracy(GNB_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB_classifier accuracy:  82.53012048192771\n"
     ]
    }
   ],
   "source": [
    "BNB_classifier = SklearnClassifier(BernoulliNB()) \n",
    "BNB_classifier.train(train_set)\n",
    "print('BNB_classifier accuracy: ', nltk.classify.accuracy(BNB_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy:  76.50602409638554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression()) \n",
    "LogisticRegression_classifier.train(train_set)\n",
    "print('LogisticRegression_classifier accuracy: ', nltk.classify.accuracy(LogisticRegression_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy:  75.75301204819277\n"
     ]
    }
   ],
   "source": [
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier()) \n",
    "SGDClassifier_classifier.train(train_set)\n",
    "print('SGDClassifier_classifier accuracy: ', nltk.classify.accuracy(SGDClassifier_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC_classifier = SklearnClassifier(SVC()) \n",
    "# SVC_classifier.train(train_set)\n",
    "# print('SVC_classifier accuracy: ', nltk.classify.accuracy(SVC_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC_classifier accuracy:  77.40963855421687\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier = SklearnClassifier(NuSVC()) \n",
    "NuSVC_classifier.train(train_set)\n",
    "print('NuSVC_classifier accuracy: ', nltk.classify.accuracy(NuSVC_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy:  76.50602409638554\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC()) \n",
    "LinearSVC_classifier.train(train_set)\n",
    "print('LinearSVC_classifier accuracy: ', nltk.classify.accuracy(LinearSVC_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember\n",
    "\n",
    ">These all above algos(classifiers) have got their own parameters, which we can tweak and imrove our accuracy, currently here we have used the default values for each of them.\n",
    ">\n",
    ">If you want to know more about those params, you can check those out on sklearn website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voted_classifier accuracy:  81.17469879518072\n"
     ]
    }
   ],
   "source": [
    "voted_classfier = VoteClassifier(#classifier, \n",
    "                                 MNB_classifier, \n",
    "                                 BNB_classifier,  \n",
    "                                 #SGDClassifier_classifier, \n",
    "                                 LinearSVC_classifier, \n",
    "                                 NuSVC_classifier,\n",
    "                                 LogisticRegression_classifier)\n",
    "print('voted_classifier accuracy: ', nltk.classify.accuracy(voted_classfier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Classfication: \", voted_classfier.classify(test_set[0][0]), \", Confidence: \", voted_classfier.confidence(test_set[0][0]) * 100)\n",
    "# print(\"Classfication: \", voted_classfier.classify(test_set[1][0]), \", Confidence: \", voted_classfier.confidence(test_set[1][0]) * 100)\n",
    "# print(\"Classfication: \", voted_classfier.classify(test_set[2][0]), \", Confidence: \", voted_classfier.confidence(test_set[2][0]) * 100)\n",
    "# print(\"Classfication: \", voted_classfier.classify(test_set[3][0]), \", Confidence: \", voted_classfier.confidence(test_set[3][0]) * 100)\n",
    "# print(\"Classfication: \", voted_classfier.classify(test_set[4][0]), \", Confidence: \", voted_classfier.confidence(test_set[4][0]) * 100)\n",
    "# print(\"Classfication: \", voted_classfier.classify(test_set[5][0]), \", Confidence: \", voted_classfier.confidence(test_set[5][0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

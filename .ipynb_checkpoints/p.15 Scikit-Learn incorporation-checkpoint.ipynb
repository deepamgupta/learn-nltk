{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sckit learn\n",
    "\n",
    "nltk is just a bunch of tools for nlp, it does not provide ML functionaltiy.\n",
    "Sckit learn does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random # this we will use to shuffle our dataset, coz for now it is higlhy ordered, first all negative, than positive than neutaral.\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "# this is basically an nltk module but it is actually a wrapper around the sklearn classifier to add its functionilities in nltk\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every single sklearn learn algorithm comes paired with a bunch of parameters you can use. The default parameters are general parameters that are pretty good but it's good to know what the parameters are and how adjusting them will change your success. \n",
    "\n",
    ">So, keep that in mind that despite the success that we get with these parameter whether it's good or bad you can probably improve that success by at least ten percent by correctly choosing and not using the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)\n",
    "# print(documents[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_what we're going to end up having to do is take all of these words though literally take every word in every review and compile them and then basically we do is we'll take that list of words and we'll find the most popular words  used and then we take of those most popular words which one appear in positive text and which ones negative text and then we simply just search for those words and whichever one it has more negative words or more positive words that's how we classify it_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in words:\n",
    "        features[w] = (w in words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = featuresets[:1900]\n",
    "test_set = featuresets[1900:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Byes Algorithm\n",
    "posterior = prior occurences * occurences / likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading classifier from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_f = open('naivebyes.pickle', 'rb')\n",
    "classifier = pickle.load(classifier_f)     \n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes algo accuracy:  71.0\n",
      "Most Informative Features\n",
      "                  avoids = True              pos : neg    =     13.0 : 1.0\n",
      "                  seagal = True              neg : pos    =     13.0 : 1.0\n",
      "                    slip = True              pos : neg    =     11.7 : 1.0\n",
      "             outstanding = True              pos : neg    =     11.2 : 1.0\n",
      "                    3000 = True              neg : pos    =     11.0 : 1.0\n",
      "             fascination = True              pos : neg    =     11.0 : 1.0\n",
      "               ludicrous = True              neg : pos    =     10.7 : 1.0\n",
      "               insulting = True              neg : pos    =     10.6 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "                  hatred = True              pos : neg    =     10.3 : 1.0\n",
      "                   sucks = True              neg : pos    =     10.2 : 1.0\n",
      "                  annual = True              pos : neg    =      9.7 : 1.0\n",
      "                  hudson = True              neg : pos    =      9.7 : 1.0\n",
      "                seamless = True              pos : neg    =      9.7 : 1.0\n",
      "                    taxi = True              pos : neg    =      9.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Original Naive Bayes algo accuracy: ', nltk.classify.accuracy(classifier, test_set) * 100)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy:  87.0\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB()) \n",
    "MNB_classifier.train(train_set)\n",
    "print('MNB_classifier accuracy: ', nltk.classify.accuracy(MNB_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNB_classifier = SklearnClassifier(GaussianNB()) \n",
    "# GNB_classifier.train(train_set)\n",
    "# print('GNB_classifier accuracy: ', nltk.classify.accuracy(GNB_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB_classifier accuracy:  86.0\n"
     ]
    }
   ],
   "source": [
    "BNB_classifier = SklearnClassifier(BernoulliNB()) \n",
    "BNB_classifier.train(train_set)\n",
    "print('BNB_classifier accuracy: ', nltk.classify.accuracy(BNB_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy:  89.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression()) \n",
    "LogisticRegression_classifier.train(train_set)\n",
    "print('LogisticRegression_classifier accuracy: ', nltk.classify.accuracy(LogisticRegression_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy:  83.0\n"
     ]
    }
   ],
   "source": [
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier()) \n",
    "SGDClassifier_classifier.train(train_set)\n",
    "print('SGDClassifier_classifier accuracy: ', nltk.classify.accuracy(SGDClassifier_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB_classifier accuracy:  87.0\n"
     ]
    }
   ],
   "source": [
    "SVC_classifier = SklearnClassifier(SVC()) \n",
    "SVC_classifier.train(train_set)\n",
    "print('BNB_classifier accuracy: ', nltk.classify.accuracy(SVC_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC_classifier accuracy:  86.0\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier = SklearnClassifier(NuSVC()) \n",
    "NuSVC_classifier.train(train_set)\n",
    "print('NuSVC_classifier accuracy: ', nltk.classify.accuracy(NuSVC_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy:  87.0\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC()) \n",
    "LinearSVC_classifier.train(train_set)\n",
    "print('LinearSVC_classifier accuracy: ', nltk.classify.accuracy(LinearSVC_classifier, test_set) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember\n",
    "\n",
    ">These all above algos(classifiers) have got their own parameters, which we can tweak and imrove our accuracy, currently here we have used the default values for each of them.\n",
    ">\n",
    ">If you want to know more about those params, you can check those out on sklearn website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
